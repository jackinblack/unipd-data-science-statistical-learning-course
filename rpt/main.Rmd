---
title: "Application of statistical analysis and modelling to Seoul Bike Sharing Demand dataset"
author: "Vasile Bezer (2124011), Davide Nieto (2126359)"
font: 10pt
output:
  pdf_document:
    toc: true
    number_sections: true
  html_document:
    toc: false
  word_document:
    toc: false
---

\

# Introduction

This project presents a statistical analysis of the relationship between the number of rented bikes in a day and a wide range of atmospherical variables and time variables.
To explore this relationship we will implement a poisson regression, a widely used model in the context of count data. The data we will use is from the Seoul Bike Sharing Demand dataset which incorporates real world bike sharing count of the city of Seoul with weather context ([the dataset can be retreived at this link https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand](https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand)).
As major cities worldwide are moving towards smart mobility we hope to gain insight into how open-vehicle sharing systems could be optimized for efficiency
through out the year by finding relationships between number of shares and weather.

# Dataset insight

We proceed our in depth look at the dataset by importing and looking at the summary. The format of the dataset is csv, to read it
we use the built-in function read.csv of R. We decide to use a naming convention for our predictors in order to avoid misinterpetations of special characters present in the dataset file.

```{r}
data.col.names = c("Date", "Rented.Bike.Count", "Hour", "Temperature"
	,"Humidity", "Wind.speed", "Visibility", "Dew.point.temperature"
	,"Solar.Radiation", "Rainfall", "Snowfall", "Seasons"
	,"Holiday", "Functioning.Day");
data <- read.csv(
	file = "../data/SeoulBikeData.csv"
	,sep = ","
	,check.names = FALSE
	,col.names = data.col.names
);
nrow(data); ncol(data); summary(data);
```

The proposed dataset consists of 1 target, namely Rented Bike count, and 13 predictors whose characteristics are described by 8760 rows.
The description of each predictor is given by the dataset's source as follows:

| Variable Name         | Description                                          | Role    | Type        | Units | Missing Values |
| :-------------------- | :--------------------------------------------------- | :------ | :---------- | :---: | :------------- |
| Date                  | year-month-day                                       | Feature | Date        |       | no             |
| Rented Bike count     | Count of bikes rented at each hour                   | Feature | Integer     |       | no             |
| Hour                  | Hour of the day                                      | Feature | Integer     |       | no             |
| Temperature           | Temperature in Celsius                               | Feature | Continuous  |   C   | no             |
| Humidity              | %                                                    | Feature | Integer     |   %   | no             |
| Windspeed             | m/s                                                  | Feature | Continuous  |  m/s  | no             |
| Visibility            | 10m                                                  | Feature | Integer     |  10m  | no             |
| Dew point temperature | Celsius                                              | Feature | Continuous  |   C   | no             |
| Solar radiation       | MJ/m2                                                | Feature | Continuous  | Mj/m2 | no             |
| Rainfall              | mm                                                   | Feature | Integer     |  mm   | no             |
| Snowfall              | cm                                                   | Feature | Integer     |  cm   | no             |
| Seasons               | Winter, Spring, Summer, Autumn                       | Feature | Categorical |       | no             |
| Holiday               | Holiday/No holiday                                   | Feature | Binary      |       | no             |
| Functional Day        | NoFunc (Non-Functional Hours), Fun(Functional hours) | Feature | Binary      |       | no             |

## Missing value imputation

A key part of data preprocessing is dealing with missing values. These gaps can happen due to incomplete information or other reasons.
If not handled properly, they can result in incorrect predictions. It's important to find and fix any missing values.
We notice that none of the predictors are reported as missing in the dataset's source, we proceed by checking this fact:

```{r}
ceiling( colSums(is.na(data)) / nrow(data) * 100 );
```
In our case this dataset doesn't present this problem.

Here we also want to make sure that the dataset is coherent with itself. Specifically, we want to make sure that when the system is not operative there are no erroneous shares.
```{r}
sum(data$Rented.Bike.Count[data$Functioning.Day == "No"]);
sum(data$Rented.Bike.Count[data$Functioning.Day == "Yes"]);
```

Notice, also in the summary, how our Humidity predictor sometimes is reported as 0(%), since this scenario is highly unlikely
in a real world scenario we assume that these values are attributed to a malfunctioning of the devices responsible for this predictor.
We will adopt an interpolating technique to address this issue in the following paragraph.

## Data preprocessing

As our first step, we will select for our analysis only those hours where the app is functioning, since on the hours where it doesn't we are going to observe 0 rented bikes independently from the weather.

```{r}
data <- data[data$Functioning.Day=="Yes",]
```

Our dataset contains the following categorical variables: Seasons, Holiday, and Functioning Day. As seen before R treats them as character variables but we need to transform them into factors for R to interpret them correctly. Since as we have seen before we selected only the observation where Functioning Day has a "Yes" value, this procedure is only done for Seasons and Holiday.

```{r}
data$Seasons 		<- as.factor( data$Seasons );
data$Holiday 		<- as.factor( data$Holiday );
```

Transforming everything into factors servers the following purpose

We also need to transform our predictors Date and Hour into a date format correctly interpretable by R.

```{r}
data$tmstmp <- strptime(
	paste( data$Date, data$Hour )
	,format="%d/%m/%Y %H"
);
data$Date <- as.Date( data$Date, format="%d/%m/%Y" );
data$tmstmp <- as.Date( data$tmstmp );
```

As described in the above paragraph, we proceed by replacing the missing humidity values with the median of the remaining correct values.

```{r}
median.humidity <- median(data$Humidity[data$Humidity != 0], na.rm = TRUE);
data$Humidity[data$Humidity == 0] <- median.humidity;
```

```{r}
# data$Hour<- as.factor(data$Hour);

# this breaks the for loop next

```


# Exploratory Data analysis

Exploratory Data Analysis (EDA) is an important step in understanding data through summary statistics and visualizations. Its main purpose is to get a clear picture of the data before modelling. EDA helps identify patterns and trends, showing how data can be effectively used. It also sparks new research ideas by revealing unexpected relationships. Additionally, EDA helps find errors
and inconsistencies. It checks the basic assumptions needed for further analysis by looking at the distribution and variety of data.

In the next paragraphs we push to achieve the following:

- learn variable distributions and suitability and for the poisson regression model
- learn dependencies and patterns between variables

## Distributions

By plotting the distribution histograms we hope to identify the shape, central tendency, and spread of data, revealing patterns, outliers, and skewness of our predictors.

```{r}
par(mfrow = c(3, 4));
for (i in 2:11) {
	hist(data[[i]], right = FALSE, probability = TRUE
		,main = paste("Histogram of", names(data)[i])
		,xlab = names(data)[i], col = "skyblue", border = "white", cex.main = 0.8);
	lines(density(data[[i]]), col = "darkblue", lwd = 2);
	var.mean <- mean(data[[i]], na.rm = TRUE);
	var.median <- median(data[[i]], na.rm = TRUE);
	unique.out <- unique(data[[i]]);
	var.mode <- unique.out[which.max(tabulate(match(data[[i]], unique.out)))];

	abline(v = var.mean, col = "#FF6347", lty = 1); # red
	abline(v = var.median, col = "#4682B4", lty = 1); # blue
	abline(v = var.mode, col = "#3CB371", lty = 1); # green
}
# par(mfrow = c(1, 1));
```

The target follows a poisson distribution...

## Correlation matrix

In order to find linear relationships between our continuous predictors our first step is to build
a correlation matrix.

```{r}
library(corrplot, warn.conflicts = FALSE); corrplot(
	cor(data[c(2, 4:11)])
	,method = "number"
	,diag = FALSE
	,tl.cex = 0.8
	,number.cex = 0.6
	,tl.col = "black"
	);
```

The most important predictors, in linear terms, for our target seem to be Temperature, Hour and Dew point temperature. We can also
notice how Temperature and Dew point temperature are highly correlated which signifies that there might be
a confounding effect taking place.
In the next paragraph, our objective is to investigate the most siginficant correlations with our target.

## Partial correlation graph

Partial correlation allows us to investigate the presence of linear relationships between variables with the effect of a set of controlling random
variables removed. This in turn allows us to capture relationships more in depth compared to the classic correlation matrix. We proceed to build this graph
by using the treshold method.

```{r}
library(igraph, warn.conflicts = FALSE); library(gRbase, warn.conflicts = FALSE);

threshold 				<- 1/5;
covariance.matrix 		<- var( data[2:11], use="pairwise.complete.obs" );
concentration.matrix 	<- -cov2cor( solve(covariance.matrix) );
adjacency.matrix 		<- abs( concentration.matrix ) > threshold;
diag(adjacency.matrix) 	<- 0; # remove self dependencies
adjacency.graph 		<- as( adjacency.matrix, "igraph" );

plot( adjacency.graph );
```

The edge between our target and Hour suggests that there is a direct conditional dependency between these two variables.
This means that knowing the value of Hour provides information about Rented.Bike.Count beyond what is explained by the other variables in the dataset.
On the contrary, Snowfall and Rainfall seem to be independent but this alone does not imply we should remove
these variables from our analysis.

## Scatter plots

We now try plotting our target, in our case Rented Bike count, against the new Date variable we created
to see if we can notice any trends. With the help of a legend we split the dataset into all 4 seasons.

```{r}
palette = c("Spring" = "#78C850", "Summer" = "#FFB14E"
	,"Autumn" = "#DC143C", "Winter" = "#4B8BBE");
plot(data$tmstmp, data$Rented.Bike.Count
	,col = palette[data$Season], pch = 16
	,cex = 0.5, main = "Bike shares vs. Date"
	,xlab = "Date",ylab = "Rented Bike count" );
legend("topleft", legend = names(palette), col = palette
	,pch = 16, title = "Season", cex = 0.7);
lines(loess.smooth(data$tmstmp, data$Rented.Bike.Count), lty = 1, col = 1);
```

Here we notice how bike share demand peaks in summer and declines in winter implying that the bike
shares are high when the weather is warm.

```{r}
palette <- colorRampPalette(c("darkblue", "skyblue", "orange", "red", "darkblue"))(24);
plot(data$tmstmp, data$Rented.Bike.Count
	,col = palette[data$Hour], pch = ifelse(data$Holiday == "No Holiday", 16, 3)
	,cex = 0.5, main = "Bike shares vs. Date"
	,xlab = "Date", ylab = "Rented Bike count");
legend("topleft", ncol = 3, legend = seq(1,24), col = palette
	,pch = 16, title = "Hour", cex = 0.7);
lines(loess.smooth(data$tmstmp, data$Rented.Bike.Count), lty = 1, col = 1);
```

This plot allows us to notice that although the target is indeed influenced by Hour, the influence is not as
strong as for Temperature conforming with the result obtained in the correlation matrix.

We now proceed to plot our target against Temperature to see if our assumptions have any foundation.

```{r}
palette = c("Spring" = "#78C850", "Summer" = "#FFB14E"
	,"Autumn" = "#DC143C", "Winter" = "#4B8BBE");
plot(data$Temperature, data$Rented.Bike.Count
	,col = palette[data$Season], pch = ifelse(data$Holiday == "No Holiday", 16, 3)
	,cex = 0.5, main = "Bike shares vs. Temperature"
	,xlab = "Temperature", ylab = "Rented Bike count");
legend("topleft", legend = names(palette), col = palette
	,pch = 16, title = "Season", cex = 0.7);
lines(loess.smooth(data$Temperature, data$Rented.Bike.Count), lty = 1, col = 1);
```

This plot shows us how bike shares are indeed influenced by temperature in a mostly linear manner.
Although the correlation between target and Hour is relatively significant, by inspecting the last two scatter plots
we cannot conclude that this is true as we can clearly see that we have peaks even at late hours.

```{r}
palette <- colorRampPalette( c("#1D4E89", "#D0E1F9", "#1D4E89") )(24);
colors <- palette[data$Hour + 1]
plot(data$Hour, data$Rented.Bike.Count
	,col = colors
	,xlab = "Hour", ylab = "Rented Bike count"
	,main = "Bike shares vs. Hour"
	,pch = 16, cex = 0.7);
```

In this plot we can notice how the correlation between the target and Hour is explained by shares peak during rush hour

```{r}
palette = c("Spring" = "#78C850", "Summer" = "#FFB14E"
	,"Autumn" = "#DC143C", "Winter" = "#4B8BBE");
plot(data$Humidity, data$Rented.Bike.Count
	,col = palette[data$Season], pch = ifelse(data$Holiday == "No Holiday", 16, 3)
	,cex = 0.5, main = "Bike shares vs. Humidity"
	,xlab = "Humidity", ylab = "Rented Bike count");
legend("topleft", legend = names(palette), col = palette
	,pch = 16, title = "Season", cex = 0.7);
lines(loess.smooth(data$Humidity, data$Rented.Bike.Count), lty = 1, col = 1);
```

# Modelling

## Variables of interest

## Poisson distribution and Poisson regression model

If the discrete random variable ${Y}$ has Poisson distribution with intensity or rate parameter ${\mu}$, ${\mu}$ > 0, then ${Y}$ has the density:

$${\Pr(Y = y) = \frac{e^{-\mu} \mu^y}{y!}, \quad y = 0, 1, \ldots}$$

where ${E(Y) = \operatorname{Var}(Y) = \mu}$. Equality of mean and variance of Poisson distribution is referred to as the equi-dispersion property
of Poisson, which is mostly violated in real life data.

The Poisson regression is a generalized linear model (GLM) mainly used for count data and contingency tables. GLMs are characterized by three
essential components:

1. The data ${y = \{y_1, y_2, \ldots, y_n\}}$ are assumed to be independently distributed with means:
   $${\mu = \{\mu_1, \mu_2, \ldots, \mu_n\}}$$

2. The (systematic) linear model predicts:
   ${Y = \{Y_1, Y_2, \ldots, Y_n\}}$,
   from parameters ${\beta}$ via a known matrix of coefficients ${X}$
   $${Y = X\beta}$$

3. Finally, ${\mu}$ is related to ${Y}$ by a linking function: $${\mu = f(Y)}$$

Component (2) is the same for all GLM while the other components vary. In a Poisson regression we have at component (1) a Poisson distribution,
and at component (3) ${\mu = e^Y}$ as a link function.

## Offset and exposure

Poisson regression can also be suitable for rate data, where the rate is a count of events divided by some measure of that unit's exposure.
More generally, event rates can be calculated as events per unit time, which allows the observation window to vary for each unit. In our project,
this variability arises due to differences in possible rentable bikes across days. To account for this difference an offset could be employed.
An offset is a fixed known value that is added to the linear predictor in the model. It is used when you have prior knowledge or a fixed
adjustment factor that needs to be accounted for in the model. Let's see how it works in practice:

$${\log\left(\frac{E(y)}{t}\right) = b_0 + b_1 x}$$

where ${y}$ = dependent variable (count); ${{E(y)}}$ = Expected count value; ${x}$ = independent variable; ${b_0, b_1}$ are the regression
coefficients; and ${\frac{E(y)}{t}}$ = rate (i.e., count/exposure). The equation is then re-written as:

$${\log(E(y)) - \log(t) = b_0 + b_1 x}$$
$${\log(E(y)) = b_0 + b_1 x + \log(t)}$$

where ${\log(t)}$ is the offset of the variable. In our project we decided not to employ an offset because the period we take into
consideration is only 1 year, and there wasn't a significant increase of available bikes during this period.

## Poisson model
The first model we created was a poisson regression where Rented.Bike.Count is the dependent variable, and all the other columns in the pre-processed dataset are the indepent variables.
```{r}
data<-data[,c(-1, -14)]
data$Hour<- as.factor(data$Hour)

model <- glm(Rented.Bike.Count ~., family = poisson(link = "log"), data=data)
summary(model)
library(car)
vif(model)
```
From the summary we can see that all the variables came out as significant. Since our exploratory data analysis made us suspect that collinearity might be present we also decided to run a Variance Inflation Factor analysis. From it we can see how Temperature and Dew.point.temperature have a high VIF value. In our second model we decided to refit a poisson regression but without Dew.point.temperature this time. Here's the result:

```{r}
model1 <- glm(Rented.Bike.Count~.-Dew.point.temperature, family = "poisson", data=data)
summary(model1)
vif(model1)
```
We can see that by eliminating Dew.point.temperature we see a significant improvement in the VIF results. The resulting model shows positive coefficient for Hour7 to Hour9, and for Hour15 to Hour23 with Hour24 as the benchmark factor. Temperature, Solar.Radiation, and Holiday also exhibit a positive coefficient, meaning that the model will predict a higher value for the dependent variable as the covariate value increases, with a small p-value indicating high significance. All the other variables have a negative coefficients meaning that the model will predict a lower value for the dependent variable as the covariate value increases. This values confirm what we saw in the exploratory data analysis.

## Diagnostics
Now we will plot the diagnostic plots for model 1:
```{r}
par(mfrow = c(2, 2))
plot(model1)
```
The results from these graphs show that the model is having some issues fitting to the data. This is maybe due to outliers, overdispersion/underdispersion or the lack of an offset.

A limitation of the Poisson model is its implicit assumption that the variance of ${y_i}$ equals its mean.
This is a restrictive property and often fails to hold in practice as there is over dispersion
in the data. When the dependent variable is over-dispersed, e.g. the variance of average daily
ridership data is greater than its mean, the Poisson regression model cannot be employed

```{r}
par(mfrow = c(1, 3))
plot(predict(model1, type = "link"), rstandard(model1))
abline(h = 0)
plot(predict(model1, type = "response"), rstandard(model1))
abline(h = 0)
qqnorm(rstandard(model1))
qqline(rstandard(model1))
```

```{r}
deviance(model)
deviance(model1)

model.residual.deviance <- model$deviance
model1.residual.deviance <- model1$deviance

model
model1
```

```{r}
mean_rented_bike_count <- mean(data$Rented.Bike.Count)
var_rented_bike_count <- var(data$Rented.Bike.Count)

# Print the mean and variance
print(paste("Mean:", mean_rented_bike_count))
print(paste("Variance:", var_rented_bike_count))
```

## Model comparisons

```{r}
anova(model, model1, test = "Chisq")
```

# Conclusions

This project undertook a statistical examination of the intricate relationship between the number of rented bikes in a particular hour, and the weather present at the same time. The analysis incorporated a comprehensive dataset encompassing various meteorological factors, alongside "Rented Bike count".
Exploratory data analysis reinforced the notion of a significant correlation between the weather experienced, the hour of the day, and the number of bikes rented. The visual correlations we saw were mostly confirmed by our Poisson model.
However our analysis it's not without limitations:
Offset: While in our project we assumed that the lack of an offset wouldn't affect our models too much, this might have been a faulty assumption.
Temporal considerations: our project did not present a time series analysis, and failing to account for it, could have a significant effect on the results.
Equidispersion: The poisson model we implemented assumes the equidispersion of our data. This assumption is often violated with real-life data and future research should look into it.

In conclusion, this project has contributed valuable insights into the intricate connection between the number of rented bikes in a particular hour, and the weather present at the same time. Addressing the identified limitations in future research will help us understand these relationships even further, and make it more possible to formulate statements on the role of the weather with more confidence than the one we could use in this conclusion.
